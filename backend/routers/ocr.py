# backend/routers/ocr.py
# ============================================
# í™”ì¥í’ˆ OCR ë¶„ì„ (MVP Router ë²„ì „)
# - í”„ë¡œí† íƒ€ì…ì˜ OCR ë¡œì§ì„ ê·¸ëŒ€ë¡œ í¬í•¨
# - FastAPI ë¼ìš°í„°ë¥¼ í•¨ê»˜ ì •ì˜
# ============================================

from fastapi import APIRouter, UploadFile, File, Form, HTTPException, Body
from fastapi.responses import JSONResponse
import os
import io
import re
import difflib
import tempfile
from typing import Dict, List, Optional, Any

from dotenv import load_dotenv, find_dotenv
from google.cloud import vision
from PIL import Image  # ì‚¬ìš© ê°€ëŠ¥ì„± ëŒ€ë¹„
from sqlalchemy import create_engine, text
from sqlalchemy.engine import Engine
from urllib.parse import quote_plus

router = APIRouter(prefix="/ocr", tags=["ocr"])

# ============================================
# DB ì—°ê²° (í”„ë¡œí†  ë™ì¼)
# ============================================
def get_engine() -> Engine:
    load_dotenv()
    dialect = os.getenv("DB_DIALECT", "{DB_DIALECT}")
    host    = os.getenv("DB_HOST", "{DB_HOST}")
    port    = os.getenv("DB_PORT", "{DB_PORT}")
    user    = os.getenv("DB_USER", "{DB_USER}")
    pw      = os.getenv("DB_PASSWORD", "{DB_PASSWORD}")
    name    = os.getenv("DB_NAME", "{DB_NAME}")
    dsn = f"{dialect}://{quote_plus(user)}:{quote_plus(pw)}@{host}:{port}/{quote_plus(name)}?charset=utf8mb4"
    return create_engine(dsn, pool_pre_ping=True, future=True)

# ============================================
# OCR + ê²€ì¦ (í”„ë¡œí†  ë™ì¼)
# ============================================
def extract_text_from_image(image_path: str) -> Optional[str]:
    try:
        # 1) .env ìˆìœ¼ë©´ ë¡œë“œ, ì—†ì–´ë„ í†µê³¼
        base_dir = ""
        try:
            dotenv_path = find_dotenv()
            if dotenv_path:
                load_dotenv(dotenv_path)
                base_dir = os.path.dirname(dotenv_path)
        except Exception:
            pass  # .env ê°•ì œ ì˜ì¡´ ì œê±°

        # 2) í™˜ê²½ë³€ìˆ˜ ìš°ì„ 
        json_path = (os.getenv("GOOGLE_APPLICATION_CREDENTIALS") or "").strip()
        if not json_path:
            raise Exception("GOOGLE_APPLICATION_CREDENTIALS not set")

        # 3) ìƒëŒ€ê²½ë¡œë©´ .env ê¸°ì¤€ìœ¼ë¡œ ë³´ì •
        if not os.path.isabs(json_path) and base_dir:
            json_path = os.path.join(base_dir, json_path)

        if not os.path.exists(json_path):
            raise Exception(f"ì„œë¹„ìŠ¤í‚¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {json_path}")

        client = vision.ImageAnnotatorClient.from_service_account_json(json_path)
        with io.open(image_path, "rb") as f:
            content = f.read()
        image = vision.Image(content=content)
        resp = client.document_text_detection(image=image)
        if resp.error.message:
            raise Exception(f"Vision API ì˜¤ë¥˜: {resp.error.message}")
        return resp.full_text_annotation.text
    except Exception as e:
        print(f"OCR ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return None

def validate_cosmetic_image(ocr_text: str) -> Dict[str, Any]:
    if not ocr_text or len(ocr_text.strip()) < 10:
        return {
            "is_valid": False, "has_text": False,
            "error_message": "í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ì‚¬ì§„ì…ë‹ˆë‹¤.", "match_count": 0
        }
    cosmetic_keywords = [
        "í™”ì¥í’ˆ","í¬ë¦¼","ë¡œì…˜","ì—ì„¼ìŠ¤","ì„¸ëŸ¼","í† ë„ˆ","ìŠ¤í‚¨","ì—ë©€ì „","í´ë Œì§•",
        "ë§ˆìŠ¤í¬","íŒ©","ì„ í¬ë¦¼","íŒŒìš´ë°ì´ì…˜","ì¿ ì…˜","ë¦½ìŠ¤í‹±","ìƒ´í‘¸","ë¦°ìŠ¤","ë°”ë””","í–¥ìˆ˜",
        "ìš©ëŸ‰","ml","g","ì„±ë¶„","ì‚¬ìš©ë²•","ì œì¡°","ìœ í†µê¸°í•œ","í™”ì¥í’ˆì œì¡°ì—…ì","í™”ì¥í’ˆì±…ì„íŒë§¤ì—…ì",
        "ì‹ì•½ì²˜","ì „ì„±ë¶„","ingredients"
    ]
    mc = sum(1 for k in cosmetic_keywords if k.lower() in ocr_text.lower())
    return {
        "is_valid": mc >= 1, "has_text": True, "match_count": mc,
        "error_message": None if mc >= 1 else "í™”ì¥í’ˆ ì‚¬ì§„ì´ ì•„ë‹™ë‹ˆë‹¤."
    }

# ============================================
# ë¶„ì„ê¸° (í”„ë¡œí†  ë™ì¼)
# ============================================
class CosmeticAnalyzer:
    def __init__(self):
        self.engine = get_engine()

    def analyze_from_text(self, ocr_text: str) -> Optional[Dict[str, Any]]:
        validation = validate_cosmetic_image(ocr_text)
        if not validation["is_valid"]:
            pass

        lines = [line.strip() for line in ocr_text.splitlines() if line.strip()]
        product_candidates: List[str] = []
        STOP = ["ì‚¬ìš©","íŒí”„","ê³µê¸°","ë¶„ë¦¬ë°°ì¶œ","í”Œë¼ìŠ¤í‹±","ì „ì„±ë¶„","ì£¼ì˜","ì œì¡°","ìš©ëŸ‰","ml","ë°©ë²•","í”¼ë¶€","ê³ ë¯¼"]
        for line in lines[:10]:
            if len(line) < 3 or len(line) > 60:
                continue
            if any(k in line for k in STOP):
                continue
            if len(re.findall(r"[ê°€-í£a-zA-Z]", line)) > len(line) * 0.5:
                product_candidates.append(line)

        product_data = None
        clean_search_text = " ".join(product_candidates)
        print(f"[DEBUG] FTS Search Text: '{clean_search_text}'")
        if clean_search_text:
            product_data = self._fuzzy_search_product(clean_search_text)

        if not product_data:
            print("[DEBUG] FTS Failed. Falling back to LIKE search...")
            for c in product_candidates:
                product_data = self._search_product_by_name(c, use_fts=False)
                if product_data:
                    break

        if not product_data:
            ocr_ingredients = self._extract_ingredients_from_ocr(ocr_text)
            caution = self._query_caution_ingredients(ocr_ingredients)
            return {
                "source":"ocr_direct_analysis",
                "product_name":None,"brand":None,"price_krw":None,"capacity":None,"image_url":None,
                "ingredients": ocr_ingredients, "caution_ingredients": caution,
                "ocr_text": ocr_text, "validation": validation,
                "error":"ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì œí’ˆì„ ì°¾ì§€ ëª»í•´ OCR í…ìŠ¤íŠ¸ë¡œ ì„±ë¶„ë§Œ ë¶„ì„í•©ë‹ˆë‹¤."
            }

        caution = self._query_caution_ingredients(product_data.get("ingredients", []))
        return {
            "source":"database",
            "product_name": product_data.get("product_name"),
            "brand": product_data.get("brand"),
            "price_krw": product_data.get("price_krw"),
            "capacity": product_data.get("capacity"),
            "image_url": product_data.get("image_url"),
            "ingredients": product_data.get("ingredients", []),
            "caution_ingredients": caution,
            "ocr_text": ocr_text,
            "validation": validation
        }

    def _extract_ingredients_from_ocr(self, ocr_text: str) -> List[str]:
        try:
            m = re.search(r"ì „ì„±ë¶„|ingredients", ocr_text, re.IGNORECASE)
            if m:
                s = ocr_text[m.end():].strip(": \n")
            else:
                s = ocr_text
            return [ing.strip() for ing in re.split(r"[,/\n]", s) if ing.strip() and len(ing.strip()) > 1]
        except Exception:
            return []

    def analyze_from_product_name(self, product_name: str) -> Optional[Dict[str, Any]]:
        pdata = self._search_product_by_name(product_name, use_fts=True)
        if not pdata:
            return None
        caution = self._query_caution_ingredients(pdata.get("ingredients", []))
        return {
            "source":"database",
            "product_name": pdata.get("product_name"),
            "brand": pdata.get("brand"),
            "price_krw": pdata.get("price_krw"),
            "capacity": pdata.get("capacity"),
            "image_url": pdata.get("image_url"),
            "ingredients": pdata.get("ingredients", []),
            "caution_ingredients": caution,
            "ocr_text": None,
            "validation": {"is_valid": True, "has_text": True, "match_count": 0}
        }

    def _search_product_by_name(self, product_name: str, use_fts: bool=True) -> Optional[Dict[str, Any]]:
        try:
            with self.engine.connect() as conn:
                result = None
                if use_fts:
                    q_fts = text("""
                        SELECT product_name,brand,image_url,price_krw,capacity,ingredients,
                               MATCH(product_name) AGAINST(:name IN NATURAL LANGUAGE MODE) AS relevance_score
                        FROM product_data
                        WHERE MATCH(product_name) AGAINST(:name IN NATURAL LANGUAGE MODE)
                        ORDER BY relevance_score DESC
                        LIMIT 1
                    """)
                    r = conn.execute(q_fts, {"name": product_name}).fetchone()
                    if r and r[6] > 0.5:
                        result = r
                if not result:
                    q_like = text("""
                        SELECT product_name,brand,image_url,price_krw,capacity,ingredients
                        FROM product_data
                        WHERE product_name LIKE :name
                        LIMIT 1
                    """)
                    r = conn.execute(q_like, {"name": f"%{product_name}%"}).fetchone()
                    result = r
                if result:
                    return {
                        "product_name": result[0], "brand": result[1], "image_url": result[2],
                        "price_krw": result[3], "capacity": result[4],
                        "ingredients": result[5].split(",") if result[5] else []
                    }
                return None
        except Exception as e:
            print(f"DB ê²€ìƒ‰ ì˜¤ë¥˜ (_search_product_by_name): {e}")
            return None

    def _fuzzy_search_product(self, clean_search_text: str) -> Optional[Dict[str, Any]]:
        try:
            with self.engine.connect() as conn:
                q = text("""
                    SELECT product_name,brand,image_url,price_krw,capacity,ingredients,
                           MATCH(product_name) AGAINST(:text IN NATURAL LANGUAGE MODE) AS relevance_score
                    FROM product_data
                    WHERE MATCH(product_name) AGAINST(:text IN NATURAL LANGUAGE MODE)
                    ORDER BY relevance_score DESC
                    LIMIT 5
                """)
                rows = conn.execute(q, {"text": clean_search_text}).fetchall()
                if not rows:
                    return None
                best = None
                best_ratio = 0.0
                base = clean_search_text.lower()
                for row in rows:
                    name_l = row[0].lower()
                    ratio = difflib.SequenceMatcher(None, base, name_l).ratio()
                    if ratio > best_ratio:
                        best_ratio = ratio
                        best = row
                if best and best_ratio >= 0.6:
                    print(f"[DEBUG] FTS Best Match Found (SimRatio: {best_ratio:.0%})")
                    return {
                        "product_name": best[0], "brand": best[1], "image_url": best[2],
                        "price_krw": best[3], "capacity": best[4],
                        "ingredients": best[5].split(",") if best[5] else []
                    }
                else:
                    print(f"[DEBUG] FTS Failed (Best SimRatio {best_ratio:.0%} < 60%)")
                    return None
        except Exception as e:
            print(f"í¼ì§€ ê²€ìƒ‰ ì˜¤ë¥˜ (_fuzzy_search_product): {e}")
            return None

    def _query_caution_ingredients(self, ingredients: List[str]) -> Dict[str, List[Dict[str, Any]]]:
        if not ingredients:
            return {"official": [], "ml_predicted": []}
        try:
            with self.engine.connect() as conn:
                ph = ",".join([":ing"+str(i) for i in range(len(ingredients))])
                params = {f"ing{i}": ing.strip() for i, ing in enumerate(ingredients)}
                q_off = text(f"""
                    SELECT korean_name, caution_grade, description
                    FROM caution_ingredients
                    WHERE korean_name IN ({ph})
                """)
                off_rows = conn.execute(q_off, params).fetchall()
                official = [{"korean_name":r[0],"caution_grade":r[1],"description":r[2]} for r in off_rows]

                official_names = {x["korean_name"] for x in official}
                remain = [ing for ing in ingredients if ing not in official_names]
                ml_list: List[Dict[str, Any]] = []
                if remain:
                    ph2 = ",".join([":rem"+str(i) for i in range(len(remain))])
                    params2 = {f"rem{i}": ing.strip() for i, ing in enumerate(remain)}
                    q_ml = text(f"""
                        SELECT korean_name, caution_grade, description
                        FROM ML_caution_ingredients
                        WHERE korean_name IN ({ph2})
                    """)
                    try:
                        ml_rows = conn.execute(q_ml, params2).fetchall()
                        ml_list = [{"korean_name":r[0],"caution_grade":r[1],"description":r[2]} for r in ml_rows]
                    except Exception as e:
                        print(f"ML ì£¼ì˜ ì„±ë¶„ ì¡°íšŒ ì˜¤ë¥˜ (ML_caution_ingredients): {e}")
                        ml_list = []
                return {"official": official, "ml_predicted": ml_list}
        except Exception as e:
            print(f"ì£¼ì˜ ì„±ë¶„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return {"official": [], "ml_predicted": []}

# ============================================
# í¬ë§¤íŒ… (í”„ë¡œí†  ë™ì¼)
# ============================================
def format_analysis_for_chat(analysis_result: Dict[str, Any]) -> Dict[str, Any]:
    """ë§ˆí¬ë‹¤ìš´ì„ ì¹´ë“œí˜•ìœ¼ë¡œ ì˜ˆì˜ê²Œ êµ¬ì„±"""
    if not analysis_result.get("success"):
        return {"text": f"âŒ {analysis_result.get('error', 'ë¶„ì„ ì‹¤íŒ¨')}", "image_url": None}

    data = analysis_result.get("data", {})
    img_url = data.get("image_url") or None
    src = data.get("source")

    out: list[str] = []

    # 1) ì œëª© + (ì„ íƒ)ì œí’ˆ ì´ë¯¸ì§€
    out.append("## ğŸ’„ í™”ì¥í’ˆ ë¶„ì„ ê²°ê³¼\n")
    if img_url:
        out.append(f"![ì œí’ˆ ì´ë¯¸ì§€]({img_url})\n")

    # 2) ì œí’ˆ ê¸°ë³¸ ì •ë³´
    if src == "database":
        name = data.get("product_name", "N/A")
        brand = data.get("brand")
        price = data.get("price_krw")
        cap   = data.get("capacity")

        out.append(f"**ì œí’ˆëª…:** {name}  ")
        if brand: out.append(f"**ë¸Œëœë“œ:** {brand}  ")
        if price: out.append(f"**ê°€ê²©:** {price:,}ì›  ")
        if cap:   out.append(f"**ìš©ëŸ‰:** {cap}  ")
    else:
        out.append("â„¹ï¸ DBì—ì„œ ì œí’ˆì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. OCR í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì„±ë¶„ë§Œ ë¶„ì„í•©ë‹ˆë‹¤.")

    out.append("\n---\n")

    # 3) ì£¼ì˜ ì„±ë¶„ ì„¹ì…˜
    caution = data.get("caution_ingredients", {}) or {}
    official = caution.get("official", []) or []
    mlp = caution.get("ml_predicted", []) or []

    # ê³µì‹ ì£¼ì˜ ì„±ë¶„
    if official:
        out.append(f"### âš ï¸ ì£¼ì˜ ì„±ë¶„ ({len(official)}ê°œ)\n")
        for i, ing in enumerate(official, 1):
            name = ing.get("korean_name", "N/A")
            grade = ing.get("caution_grade", "N/A")
            desc = (ing.get("description") or "").strip()
            out.append(f"**{i}ï¸âƒ£ {name}** (ë“±ê¸‰: {grade})  ")
            if desc:
                out.append(f"> {desc}")
            out.append("")  # ì¤„ë°”ê¿ˆ
        out.append("\n")
    else:
        out.append("### âœ… ê³µì‹ ì£¼ì˜ ì„±ë¶„ ì—†ìŒ\n")

    # ML ì˜ˆì¸¡ ì„±ë¶„
    if mlp:
        out.append(f"### ğŸ“Š ì¶”ê°€ë¡œ ì•Œì•„ë‘ë©´ ì¢‹ì„ ì„±ë¶„ (AI ì˜ˆì¸¡) ({len(mlp)}ê°œ)")
        out.append("*ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë¡œ ì˜ˆì¸¡ëœ ë¹„ì•ˆì „/ì£¼ì˜ ì„±ë¶„ì…ë‹ˆë‹¤.*\n")
        for i, ing in enumerate(mlp, 1):
            name = ing.get("korean_name", "N/A")
            grade = ing.get("caution_grade", "N/A")
            desc = (ing.get("description") or "").strip()
            out.append(f"- **{name}** (ì˜ˆì¸¡ ë“±ê¸‰: {grade})")
            if desc:
                out.append(f"  {desc}")
        out.append("\n")

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸ§¾ ê³¼í•™ì  ìš”ì•½ (ì£¼ì˜ ì„±ë¶„ ê°œìˆ˜Â·ë“±ê¸‰Â·ì¢…ë¥˜ ê¸°ë°˜)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _grade_to_score(g: str) -> float:
        """
        caution_gradeë¥¼ 0~3 ì ìˆ˜ë¡œ ì •ê·œí™”.
        - ë¬¸ìì—´ ë“±ê¸‰ ëŒ€ì‘ + ìˆ«ì(0~10) ëŒ€ì‘
        """
        if g is None:
            return 0.0
        s = str(g).strip().lower()

        # ë¬¸ìì—´ ë“±ê¸‰ ë§µ(ì˜ˆì‹œëŠ” í”„ë¡œì íŠ¸ ìƒí™©ì— ë§ê²Œ ë³´ì • ê°€ëŠ¥)
        map_str = {
            "ì €ìœ„í—˜": 0.5, "low": 0.5, "ë‚®ìŒ": 0.5,
            "ì¤‘ê°„": 1.5, "ë³´í†µ": 1.5, "moderate": 1.5,
            "ì£¼ì˜": 2.0, "ì£¼ì˜í•„ìš”": 2.0, "warning": 2.0,
            "ê³ ìœ„í—˜": 3.0, "high": 3.0, "ìœ„í—˜": 3.0,
        }
        if s in map_str:
            return map_str[s]

        # ìˆ«ì ë“±ê¸‰(ì˜ˆ: 1~10í˜•ì‹) â†’ 0~3ë¡œ ìŠ¤ì¼€ì¼ë§
        if s.replace(".", "", 1).isdigit():
            val = float(s)
            # ë³´í¸ì ì¸ 0~10 ìŠ¤ì¼€ì¼ì„ 0~3ìœ¼ë¡œ ë³€í™˜
            return max(0.0, min(3.0, (val / 10.0) * 3.0))

        return 0.0

    def _tag_flags(name: str) -> dict:
        """ì„±ë¶„ëª… í‚¤ì›Œë“œë¡œ íŠ¹ì„± í”Œë˜ê·¸ ì¶”ì¶œ"""
        n = (name or "").lower()
        return {
            "fragrance": any(k in n for k in ["fragrance", "í–¥ë£Œ", "í¼í“¸", "ë¦¬ëª¨ë„¨", "ë¦¬ë‚ ë£°", "ì œë¼ë‹ˆì˜¬", "ì‹œíŠ¸ë¡œë„¬ë¡¤"]),
            "alcohol": any(k in n for k in ["alcohol", "ì—íƒ„ì˜¬"]),
            "acid": any(k in n for k in ["aha", "bha", "pha", "salicylic", "glycolic", "lactic", "mandelic", "ì•„í•˜", "ë¹„í•˜", "ì‚´ë¦¬ì‹¤"]),
            "retinoid": any(k in n for k in ["retinol", "retinal", "ë¹„íƒ€ë¯¼ a"]),
            "oil": any(k in n for k in ["oil", "ì˜¤ì¼", "essential oil", "ì •ìœ "]),
            "silicone": any(k in n for k in ["siloxane", "silicone", "ë””ë©”ì¹˜ì½˜", "ë””ë©”í‹°ì½˜"]),
        }

    # ê³µì‹/ML í•©ì¹˜ë˜, ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜ ë¶€ì—¬(MLì€ 0.7ë°°)
    weighted = []
    flags_acc = {"fragrance":0, "alcohol":0, "acid":0, "retinoid":0, "oil":0, "silicone":0}

    for ing in official:
        sc = _grade_to_score(ing.get("caution_grade"))
        nm = ing.get("korean_name") or ""
        flags = _tag_flags(nm)
        for k,v in flags.items():
            flags_acc[k] += int(v)
        weighted.append(("official", nm, sc))

    for ing in mlp:
        sc = _grade_to_score(ing.get("caution_grade")) * 0.7  # MLì€ ì‹ ë¢°ë„ 70%
        nm = ing.get("korean_name") or ""
        flags = _tag_flags(nm)
        for k,v in flags.items():
            flags_acc[k] += int(v)
        weighted.append(("ml", nm, sc))

    total_cnt = len(weighted)
    avg_score = (sum(x[2] for x in weighted) / total_cnt) if total_cnt else 0.0
    max_score = max([x[2] for x in weighted], default=0.0)

    # ìœ„í—˜ë„ íŒë‹¨(í‰ê·  + ìµœëŒ€ì¹˜ í•¨ê»˜ ê³ ë ¤)
    # - maxê°€ ë†’ìœ¼ë©´ êµ­ì†Œ ìê·¹ ìœ„í—˜, avgê°€ ë†’ìœ¼ë©´ ì „ë°˜ì  ë¦¬ìŠ¤í¬ ì¦ê°€
    if total_cnt == 0:
        risk_level = "ë‚®ìŒ"
    else:
        if max_score >= 2.5 or avg_score >= 2.0:
            risk_level = "ë†’ìŒ"
        elif max_score >= 1.5 or avg_score >= 1.0:
            risk_level = "ì¤‘ê°„"
        else:
            risk_level = "ë‚®ìŒ"

    # íŠ¹ì„± í”Œë˜ê·¸ ë¬¸ì¥ ìƒì„±
    flag_msgs = []
    if flags_acc["fragrance"] > 0:
        flag_msgs.append("í–¥ë£Œ/ì—ì„¼ì…œì˜¤ì¼ ì„±ë¶„ í¬í•¨")
    if flags_acc["alcohol"] > 0:
        flag_msgs.append("ì•Œì½”ì˜¬ê³„ ì„±ë¶„ í¬í•¨")
    if flags_acc["acid"] > 0:
        flag_msgs.append("AHA/BHA ë“± ê°ì§ˆ ì¼€ì–´ ì„±ë¶„ í¬í•¨")
    if flags_acc["retinoid"] > 0:
        flag_msgs.append("ë ˆí‹°ë…¸ì´ë“œ(ë¹„íƒ€ë¯¼ A ê³„ì—´) í¬í•¨")
    if flags_acc["oil"] > 0:
        flag_msgs.append("ì˜¤ì¼ ì„±ë¶„ ë‹¤ìˆ˜")
    if flags_acc["silicone"] > 0:
        flag_msgs.append("ì‹¤ë¦¬ì½˜ê³„ ì„±ë¶„ í¬í•¨")

    # ìš”ì•½ í—¤ë”
    out.append("### ğŸ§¾ ë¶„ì„ ìš”ì•½")

    # í•œì¤„ í•µì‹¬
    if total_cnt == 0:
        out.append("ê³µì‹Â·AI ì˜ˆì¸¡ ê¸°ì¤€ **ì£¼ì˜ ì„±ë¶„ì´ í™•ì¸ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤**. ì „ë°˜ì ìœ¼ë¡œ ì•ˆì „í•œ í¸ì…ë‹ˆë‹¤.\n")
    else:
        out.append(
            f"ì£¼ì˜ ì„±ë¶„ {total_cnt}ê°œ, í‰ê·  ìœ„í—˜ë„ {avg_score:.1f}/3, ìµœëŒ€ ìœ„í—˜ë„ {max_score:.1f}/3 â†’ **ì¢…í•© ìœ„í—˜ë„: {risk_level}**.\n"
        )

    # í”¼ë¶€ íƒ€ì…ë³„ ê¶Œì¥ì‚¬í•­(ê°„ë‹¨ ê·œì¹™)
    recs = []
    if risk_level == "ë†’ìŒ":
        recs.append("ë¯¼ê°ì„± í”¼ë¶€ëŠ” íŒ¨ì¹˜ í…ŒìŠ¤íŠ¸ í›„ ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
    elif risk_level == "ì¤‘ê°„":
        recs.append("ë¯¼ê°ì„±Â·ì¥ë²½ ì•½í•œ í”¼ë¶€ëŠ” ì‚¬ìš© ì „ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    if flags_acc["acid"] > 0 or flags_acc["retinoid"] > 0:
        recs.append("ê°ì§ˆ ì¼€ì–´/ë ˆí‹°ë…¸ì´ë“œ ì„±ë¶„ì´ ìˆì–´ **ì €ë… ìœ„ì£¼ ì‚¬ìš©** ë° **ìì™¸ì„  ì°¨ë‹¨**ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
    if flags_acc["fragrance"] > 0:
        recs.append("í–¥ë£Œ ì„±ë¶„ì— ë¯¼ê°í•˜ë‹¤ë©´ ë™ì¼ ì œí’ˆêµ° ë‚´ **ë¬´í–¥/ì €ìê·¹ ëŒ€ì•ˆ**ì„ ê³ ë ¤í•˜ì„¸ìš”.")
    if flags_acc["alcohol"] > 0:
        recs.append("ê±´ì„±Â·ë¯¼ê°ì„± í”¼ë¶€ëŠ” **ì•Œì½”ì˜¬ í•¨ëŸ‰**ì— ìœ ì˜í•˜ì„¸ìš”.")
    if not recs:
        recs.append("ì¼ë°˜ í”¼ë¶€ì—ëŠ” ë¬´ë‚œí•˜ì§€ë§Œ, ê°œì¸ë³„ ë¯¼ê°ë„ ì°¨ì´ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")

    out.append("â€¢ " + " ".join(recs) + ("\n" if recs else ""))

    # íŠ¹ì„± ë©”íƒ€ ì •ë³´(ìˆì„ ë•Œë§Œ)
    if flag_msgs:
        out.append("**íŠ¹ì´ì‚¬í•­:** " + ", ".join(flag_msgs) + "\n")


    return {"text": "\n".join(out), "image_url": img_url}


# ============================================
# ë©”ì¸ ì²˜ë¦¬/ê²€ìƒ‰ (í”„ë¡œí†  ë™ì¼)
# ============================================
def process_cosmetic_image(image_path: str) -> Dict[str, Any]:
    txt = extract_text_from_image(image_path)
    if not txt:
        return {"success": False, "error": "OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨", "data": None}
    analyzer = CosmeticAnalyzer()
    res = analyzer.analyze_from_text(txt)
    if res:
        return {"success": True, "error": None, "data": res}
    return {"success": False, "error": "í™”ì¥í’ˆ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "data": None}

def search_product_by_name(product_name: str) -> Dict[str, Any]:
    analyzer = CosmeticAnalyzer()
    res = analyzer.analyze_from_product_name(product_name)
    if res:
        return {"success": True, "error": None, "data": res}
    return {"success": False, "error": "ì œí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "data": None}

# ============================================
# FastAPI Endpoints (í”„ë¡ íŠ¸ì—ì„œ í˜¸ì¶œ)
# ============================================

@router.post("/upload")
async def ocr_upload(image: UploadFile = File(...)):
    if not image.content_type or not image.content_type.startswith("image/"):
        raise HTTPException(400, "image íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    # ì„ì‹œ ì €ì¥
    with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(image.filename or "")[-1]) as tmp:
        content = await image.read()
        tmp.write(content)
        tmp_path = tmp.name
    try:
        result = process_cosmetic_image(tmp_path)
        formatted = format_analysis_for_chat(result)
        return JSONResponse({
            "success": result.get("success", False),
            "markdown": formatted.get("text"),
            "image_url": formatted.get("image_url"),
            "raw": result
        })
    finally:
        try: os.remove(tmp_path)
        except: pass

@router.post("/by-name")
async def ocr_by_name(
    product_name_form: Optional[str] = Form(None),
    payload: Optional[dict] = Body(None),
):
    # JSON(product_name) ìš°ì„ , ì—†ìœ¼ë©´ form ê°’ ì‚¬ìš©
    product_name = (payload or {}).get("product_name") if payload else None
    if product_name is None:
        product_name = product_name_form

    if not product_name or not product_name.strip():
        raise HTTPException(400, "product_name is required")

    result = search_product_by_name(product_name.strip())
    formatted = format_analysis_for_chat(result)
    return JSONResponse({
        "success": result.get("success", False),
        "markdown": formatted.get("text"),
        "image_url": formatted.get("image_url"),
        "raw": result
    })

@router.get("/health")
def ocr_health():
    return {"ok": True}

@router.post("/analyze-image")
async def analyze_image_alias(image: UploadFile = File(...)):
    # /ocr/uploadì™€ ë™ì¼ ë™ì‘ì„ í•˜ë„ë¡ ê·¸ëŒ€ë¡œ ì¬ì‚¬ìš©
    return await ocr_upload(image)
